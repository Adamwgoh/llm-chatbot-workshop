{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2bd2b30-f2c1-4b86-a0b0-1a4969e4d89d",
   "metadata": {},
   "source": [
    "source: https://medium.com/intel-tech/optimize-vector-databases-enhance-rag-driven-generative-ai-90c10416cb9c\n",
    "## VectorDB: Similarity Search\n",
    "Vector databases do vector retrieval by similarity search using a distance metric (where closer means the results are more similar) such as Euclidean, dot product, or cosine similarity.\n",
    "https://www.pinecone.io/learn/vector-similarity/\n",
    "\n",
    "## Indexing mechanism\n",
    "To accelerate the retrieval process, the vector data is organized using an indexing mechanism\n",
    "- Inverted File\n",
    "- Hierarchical Navigable Small Worlds (HNSW)\n",
    "- Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "source: https://zilliz.com/learn/how-to-pick-a-vector-index-in-milvus-visual-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the available data, it is difficult to determine which job has the highest salary in 2024. The provided information only shows compensation by roles for one specific company (NodeFlair) and does not provide a comprehensive view of salaries across various jobs or industries.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "documents = SimpleDirectoryReader(\"assets/data\").load_data()\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3\", request_timeout=360.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Which job has the highest salary in 2024?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c3d6d",
   "metadata": {},
   "source": [
    "You can get more llama-index integration here: https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483acec7",
   "metadata": {},
   "source": [
    "# Basic Data Ingestion\n",
    "source: https://docs.llamaindex.ai/en/stable/understanding/loading/loading/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high level transformation API\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "vector_index.as_query_engine()\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "\n",
    "# global\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.text_splitter = text_splitter\n",
    "\n",
    "# per-index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[text_splitter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f611e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low level transformation API\n",
    "from llama_index.core import Document, SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])\n",
    "\n",
    "nodes = pipeline.run(documents=documents)\n",
    "\n",
    "# Adding metadata to your document to aid in searching\n",
    "document = Document(\n",
    "    text=\"text\",\n",
    "    metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9dfb1d",
   "metadata": {},
   "source": [
    "# Storing in a vector DB (chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3729317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import (SimpleDirectoryReader, StorageContext,\n",
    "                              VectorStoreIndex)\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# load some documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# create your index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# create a query engine and query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the meaning of life?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630d64d",
   "metadata": {},
   "source": [
    "## Storing in a Vector DB (Milvus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7175224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "connection_args = { 'uri': URI, 'token': TOKEN }\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args=connection_args,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    drop_old=True,\n",
    ").from_documents(\n",
    "    all_splits,\n",
    "    embedding=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_args=connection_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649ad98-ea7c-4232-bbcf-91b98a025d34",
   "metadata": {},
   "source": [
    "# Think and Try it Out\n",
    "1. Can you mix different embeddings in a single VectorStoreIndex instance? If you wish to support multiple embeddings in a single Vector Store, how would you do that\n",
    "2. When choosing to configure your Vector Store, Think about the embeddings and how would you store them, ie Vector Size, Embedding Types ?\n",
    "3. Are you able to improve your result by customizing your embedding strategy?\n",
    "4. Given a dataset: https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail/data\n",
    "    - Can you setup a Vector Index that ingest them and accessible through LLM? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
